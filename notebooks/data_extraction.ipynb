{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from tweety import Twitter, filters\n",
    "from geotext import GeoText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Twitter(\"session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "username = os.environ.get('twitter_username')\n",
    "password = os.environ.get('twitter_password')\n",
    "# print(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.sign_in(username=username, password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, label_text, year, pages=1, wait_time=2):\n",
    "    # Get cursor top from file\n",
    "    if os.path.exists('./cursor_top.txt'):\n",
    "        with open('./cursor_top.txt', 'r') as file:\n",
    "            cursor_top = file.read()\n",
    "    else:\n",
    "        cursor_top = None\n",
    "\n",
    "    since = f'{year}-01-01'\n",
    "    until = f'{year}-12-31'\n",
    "    temporal_range = f'since:{since} until:{until}'\n",
    "\n",
    "    language = 'es'\n",
    "\n",
    "    mention = ['@policiaecuador']\n",
    "    not_from = ['@policiaecuador']\n",
    "\n",
    "    query_mention = ' OR '.join(mention)\n",
    "    query_not_from = f'-from:{\" -from:\".join(not_from)}'\n",
    "\n",
    "    tweets = app.search(keyword=f'{query} {query_mention} {query_not_from} lang:{language} {temporal_range}',\n",
    "                        filter_=filters.SearchFilters.Latest(),\n",
    "                        pages=pages,  # page = 1 => 20 tweets aprox\n",
    "                        wait_time=wait_time,  # wait_time is in seconds\n",
    "                        # cursor=cursor_top,  # pagination cursor of the last tweet gotten\n",
    "                        )\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into_dataframe(tweets, label_text) -> pd.DataFrame:\n",
    "    id, date, text, url, location, label = [], [], [], [], [], []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        id.append(int(tweet.id))\n",
    "        date.append(tweet.date)\n",
    "        text.append(str(tweet.text).replace(\"\\n\", \" \"))\n",
    "        url.append(tweet.url)\n",
    "        label.append(label_text)\n",
    "\n",
    "        cities = GeoText(tweet.text).cities\n",
    "        if cities:\n",
    "            location.append(cities)\n",
    "        else:\n",
    "            place = tweet.place\n",
    "            if place is not None:\n",
    "                location.append(place.name)\n",
    "            else:\n",
    "                location.append([])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "\n",
    "        data={\n",
    "            \"id\": id,\n",
    "            \"date\": date,\n",
    "            \"text\": text,\n",
    "            \"url\": url,\n",
    "            \"location\": location,\n",
    "            \"label\": label,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_as_csv(dataframe, label_text, keywords, path):\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    dataframe.to_csv(f'./{path}/tweets - {label_text}.csv', index=False, quoting=2)\n",
    "\n",
    "    # save search queries\n",
    "    with open(f'./{path}/tweets - {label_text}.txt', 'w') as f:\n",
    "        for keyword in keywords:\n",
    "            f.write(keyword +  '\\n')\n",
    "\n",
    "    # To save as json\n",
    "    # dataframe.to_json(f'./{path}/tweets.json', orient='records', force_ascii=False, date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_csv(path, label_text):\n",
    "    # date = \"2023-12-30 04:29:26+00:00\"\n",
    "    # date = pd.to_datetime(date)\n",
    "    # date\n",
    "\n",
    "    test = pd.read_csv(f'./{path}/tweets - {label_text}.csv')\n",
    "    # test.date = test.date.apply(lambda x: pd.to_datetime(x))\n",
    "    test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keywords = {\n",
    "    'robo': 'robo OR robando OR robaron OR robarle OR robado OR robada OR hurto OR huraton OR hurtado OR hurtada OR asalto OR asaltado OR asaltada OR asaltaron OR atraco OR atracaron OR atracado OR atracada',\n",
    "    # 'asesinato' :'matan OR mataron OR asesinato OR asesinan OR asesinaron OR asesinado OR asesinada OR asesinadas OR asesinados OR femicidio OR sicariato OR acribillan OR acribillaron OR acribillado OR acribillada OR acribilladas OR acribillados OR homicidio',\n",
    "    # 'secuestro' :'secuestraron OR secuestro OR secuestrada OR secuestrado OR secuestradas OR secuestrados',\n",
    "    # 'terrorismo' :'terrorismo bomba OR explosivo OR bombas OR explosivos OR bombas OR explosivos OR atentados OR atentado OR terrorista OR terroristas OR tiroteo OR tiroteos',\n",
    "    # 'extorsion' :'extorsión OR extorsion OR extorsionaron OR extorsionado OR extorsionada OR extorsionadas OR extorsionados OR vacuna OR vacunas',\n",
    "}\n",
    "\n",
    "data = {}\n",
    "years = range(2021, 2024) # 2021 - 2023 años con mayor indice de delincuencia\n",
    "wait_time = 5\n",
    "pages = 2\n",
    "\n",
    "for label_text, query in search_keywords.items():\n",
    "    tweets = []\n",
    "    keywords = []\n",
    "    for year in years:\n",
    "        print('year:', year, '\\nlabel:', label_text, '\\nquery:', query)\n",
    "        result = search_tweets(query, label_text, year, pages=pages, wait_time=wait_time)\n",
    "        time.sleep(wait_time)\n",
    "        tweets.extend(result)\n",
    "        keywords.append(result.keyword)\n",
    "    data[label_text] = {}\n",
    "    data[label_text]['tweets'] = load_into_dataframe(tweets, label_text)\n",
    "    data[label_text]['keywords'] = keywords\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./../datasets/non-reviewed\"\n",
    "\n",
    "for label_text in search_keywords.keys():\n",
    "    print(data[label_text]['tweets'].info())\n",
    "\n",
    "    # to display the full text / if n it will display n characters\n",
    "    # pd.set_option('display.max_colwidth', None)\n",
    "    # display(content['tweets'].head())\n",
    "\n",
    "    # save into csv\n",
    "    save_tweets_as_csv(data[label_text]['tweets'], label_text, data[label_text]['keywords'], path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_text in search_keywords.keys():\n",
    "    read_tweets_from_csv(path, label_text=label_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
