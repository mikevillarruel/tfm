{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "# Custom transformers\n",
    "from custom_transformers import Cleaner, StopWordsRemover, Lemmatizer\n",
    "# TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download the Spanish model\n",
    "# import stanza\n",
    "# stanza.download(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'actualmente', 'acuerdo', 'adelante', 'ademas', 'ademÃƒÂ¡s', 'adrede', 'afirmÃƒÂ³', 'agregÃƒÂ³', 'ahi', 'ahora', 'ahÃƒ\\xad', 'al', 'algo', 'alguna', 'algunas', 'alguno', 'algunos', 'algÃƒÂºn', 'alli', 'allÃƒ\\xad', 'alrededor', 'ambos', 'ampleamos', 'antano', 'antaÃƒÂ±o', 'ante', 'anterior', 'antes', 'apenas', 'aproximadamente', 'aquel', 'aquella', 'aquellas', 'aquello', 'aquellos', 'aqui', 'aquÃƒÂ©l', 'aquÃƒÂ©lla', 'aquÃƒÂ©llas', 'aquÃƒÂ©llos', 'aquÃƒ\\xad', 'arriba', 'arribaabajo', 'asegurÃƒÂ³', 'asi', 'asÃƒ\\xad', 'atras', 'aun', 'aunque', 'ayer', 'aÃƒÂ±adiÃƒÂ³', 'aÃƒÂºn', 'b', 'bajo', 'bastante', 'bien', 'breve', 'buen', 'buena', 'buenas', 'bueno', 'buenos', 'c', 'cada', 'casi', 'cerca', 'cierta', 'ciertas', 'cierto', 'ciertos', 'cinco', 'claro', 'comentÃƒÂ³', 'como', 'con', 'conmigo', 'conocer', 'conseguimos', 'conseguir', 'considera', 'considerÃƒÂ³', 'consigo', 'consigue', 'consiguen', 'consigues', 'contigo', 'contra', 'cosas', 'creo', 'cual', 'cuales', 'cualquier', 'cuando', 'cuanta', 'cuantas', 'cuanto', 'cuantos', 'cuatro', 'cuenta', 'cuÃƒÂ¡l', 'cuÃƒÂ¡les', 'cuÃƒÂ¡ndo', 'cuÃƒÂ¡nta', 'cuÃƒÂ¡ntas', 'cuÃƒÂ¡nto', 'cuÃƒÂ¡ntos', 'cÃƒÂ³mo', 'd', 'da', 'dado', 'dan', 'dar', 'de', 'debajo', 'debe', 'deben', 'debido', 'decir', 'dejÃƒÂ³', 'del', 'delante', 'demasiado', 'demÃƒÂ¡s', 'dentro', 'deprisa', 'desde', 'despacio', 'despues', 'despuÃƒÂ©s', 'detras', 'detrÃƒÂ¡s', 'dia', 'dias', 'dice', 'dicen', 'dicho', 'dieron', 'diferente', 'diferentes', 'dijeron', 'dijo', 'dio', 'donde', 'dos', 'durante', 'dÃƒ\\xada', 'dÃƒ\\xadas', 'dÃƒÂ³nde', 'e', 'ejemplo', 'el', 'ella', 'ellas', 'ello', 'ellos', 'embargo', 'empleais', 'emplean', 'emplear', 'empleas', 'empleo', 'en', 'encima', 'encuentra', 'enfrente', 'enseguida', 'entonces', 'entre', 'era', 'erais', 'eramos', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estais', 'estamos', 'estan', 'estando', 'estar', 'estaremos', 'estarÃƒÂ¡', 'estarÃƒÂ¡n', 'estarÃƒÂ¡s', 'estarÃƒÂ©', 'estarÃƒÂ©is', 'estarÃƒ\\xada', 'estarÃƒ\\xadais', 'estarÃƒ\\xadamos', 'estarÃƒ\\xadan', 'estarÃƒ\\xadas', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviÃƒÂ©ramos', 'estuviÃƒÂ©semos', 'estuvo', 'estÃƒÂ¡', 'estÃƒÂ¡bamos', 'estÃƒÂ¡is', 'estÃƒÂ¡n', 'estÃƒÂ¡s', 'estÃƒÂ©', 'estÃƒÂ©is', 'estÃƒÂ©n', 'estÃƒÂ©s', 'ex', 'excepto', 'existe', 'existen', 'explicÃƒÂ³', 'expresÃƒÂ³', 'f', 'fin', 'final', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuÃƒÂ©ramos', 'fuÃƒÂ©semos', 'g', 'general', 'gran', 'grandes', 'gueno', 'h', 'ha', 'haber', 'habia', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habla', 'hablan', 'habremos', 'habrÃƒÂ¡', 'habrÃƒÂ¡n', 'habrÃƒÂ¡s', 'habrÃƒÂ©', 'habrÃƒÂ©is', 'habrÃƒ\\xada', 'habrÃƒ\\xadais', 'habrÃƒ\\xadamos', 'habrÃƒ\\xadan', 'habrÃƒ\\xadas', 'habÃƒÂ©is', 'habÃƒ\\xada', 'habÃƒ\\xadais', 'habÃƒ\\xadamos', 'habÃƒ\\xadan', 'habÃƒ\\xadas', 'hace', 'haceis', 'hacemos', 'hacen', 'hacer', 'hacerlo', 'haces', 'hacia', 'haciendo', 'hago', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayÃƒÂ¡is', 'he', 'hecho', 'hemos', 'hicieron', 'hizo', 'horas', 'hoy', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiÃƒÂ©ramos', 'hubiÃƒÂ©semos', 'hubo', 'i', 'igual', 'incluso', 'indicÃƒÂ³', 'informo', 'informÃƒÂ³', 'intenta', 'intentais', 'intentamos', 'intentan', 'intentar', 'intentas', 'intento', 'ir', 'j', 'junto', 'k', 'l', 'la', 'lado', 'largo', 'las', 'le', 'lejos', 'les', 'llegÃƒÂ³', 'lleva', 'llevar', 'lo', 'los', 'luego', 'lugar', 'm', 'mal', 'manera', 'manifestÃƒÂ³', 'mas', 'mayor', 'me', 'mediante', 'medio', 'mejor', 'mencionÃƒÂ³', 'menos', 'menudo', 'mi', 'mia', 'mias', 'mientras', 'mio', 'mios', 'mis', 'misma', 'mismas', 'mismo', 'mismos', 'modo', 'momento', 'mucha', 'muchas', 'mucho', 'muchos', 'muy', 'mÃƒÂ¡s', 'mÃƒ\\xad', 'mÃƒ\\xada', 'mÃƒ\\xadas', 'mÃƒ\\xado', 'mÃƒ\\xados', 'n', 'nada', 'nadie', 'ni', 'ninguna', 'ningunas', 'ninguno', 'ningunos', 'ningÃƒÂºn', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'nueva', 'nuevas', 'nuevo', 'nuevos', 'nunca', 'o', 'ocho', 'os', 'otra', 'otras', 'otro', 'otros', 'p', 'pais', 'para', 'parece', 'parte', 'partir', 'pasada', 'pasado', 'paÃƒÂ¬s', 'peor', 'pero', 'pesar', 'poca', 'pocas', 'poco', 'pocos', 'podeis', 'podemos', 'poder', 'podria', 'podriais', 'podriamos', 'podrian', 'podrias', 'podrÃƒÂ¡', 'podrÃƒÂ¡n', 'podrÃƒ\\xada', 'podrÃƒ\\xadan', 'poner', 'por', 'por quÃƒÂ©', 'porque', 'posible', 'primer', 'primera', 'primero', 'primeros', 'principalmente', 'pronto', 'propia', 'propias', 'propio', 'propios', 'proximo', 'prÃƒÂ³ximo', 'prÃƒÂ³ximos', 'pudo', 'pueda', 'puede', 'pueden', 'puedo', 'pues', 'q', 'qeu', 'que', 'quedÃƒÂ³', 'queremos', 'quien', 'quienes', 'quiere', 'quiza', 'quizas', 'quizÃƒÂ¡', 'quizÃƒÂ¡s', 'quiÃƒÂ©n', 'quiÃƒÂ©nes', 'quÃƒÂ©', 'r', 'raras', 'realizado', 'realizar', 'realizÃƒÂ³', 'repente', 'respecto', 's', 'sabe', 'sabeis', 'sabemos', 'saben', 'saber', 'sabes', 'sal', 'salvo', 'se', 'sea', 'seamos', 'sean', 'seas', 'segun', 'segunda', 'segundo', 'segÃƒÂºn', 'seis', 'ser', 'sera', 'seremos', 'serÃƒÂ¡', 'serÃƒÂ¡n', 'serÃƒÂ¡s', 'serÃƒÂ©', 'serÃƒÂ©is', 'serÃƒ\\xada', 'serÃƒ\\xadais', 'serÃƒ\\xadamos', 'serÃƒ\\xadan', 'serÃƒ\\xadas', 'seÃƒÂ¡is', 'seÃƒÂ±alÃƒÂ³', 'si', 'sido', 'siempre', 'siendo', 'siete', 'sigue', 'siguiente', 'sin', 'sino', 'sobre', 'sois', 'sola', 'solamente', 'solas', 'solo', 'solos', 'somos', 'son', 'soy', 'soyos', 'su', 'supuesto', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sÃƒÂ©', 'sÃƒ\\xad', 'sÃƒÂ³lo', 't', 'tal', 'tambien', 'tambiÃƒÂ©n', 'tampoco', 'tan', 'tanto', 'tarde', 'te', 'temprano', 'tendremos', 'tendrÃƒÂ¡', 'tendrÃƒÂ¡n', 'tendrÃƒÂ¡s', 'tendrÃƒÂ©', 'tendrÃƒÂ©is', 'tendrÃƒ\\xada', 'tendrÃƒ\\xadais', 'tendrÃƒ\\xadamos', 'tendrÃƒ\\xadan', 'tendrÃƒ\\xadas', 'tened', 'teneis', 'tenemos', 'tener', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengÃƒÂ¡is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenÃƒÂ©is', 'tenÃƒ\\xada', 'tenÃƒ\\xadais', 'tenÃƒ\\xadamos', 'tenÃƒ\\xadan', 'tenÃƒ\\xadas', 'tercera', 'ti', 'tiempo', 'tiene', 'tienen', 'tienes', 'toda', 'todas', 'todavia', 'todavÃƒ\\xada', 'todo', 'todos', 'total', 'trabaja', 'trabajais', 'trabajamos', 'trabajan', 'trabajar', 'trabajas', 'trabajo', 'tras', 'trata', 'travÃƒÂ©s', 'tres', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviÃƒÂ©ramos', 'tuviÃƒÂ©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tÃƒÂº', 'u', 'ultimo', 'un', 'una', 'unas', 'uno', 'unos', 'usa', 'usais', 'usamos', 'usan', 'usar', 'usas', 'uso', 'usted', 'ustedes', 'v', 'va', 'vais', 'valor', 'vamos', 'van', 'varias', 'varios', 'vaya', 'veces', 'ver', 'verdad', 'verdadera', 'verdadero', 'vez', 'vosotras', 'vosotros', 'voy', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'w', 'x', 'y', 'ya', 'yo', 'z', 'ÃƒÂ©l', 'ÃƒÂ©ramos', 'ÃƒÂ©sa', 'ÃƒÂ©sas', 'ÃƒÂ©se', 'ÃƒÂ©sos', 'ÃƒÂ©sta', 'ÃƒÂ©stas', 'ÃƒÂ©ste', 'ÃƒÂ©stos', 'ÃƒÂºltima', 'ÃƒÂºltimas', 'ÃƒÂºltimo', 'ÃƒÂºltimos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476701687070998535</td>\n",
       "      <td>2021-12-30 23:48:12+00:00</td>\n",
       "      <td>#ATENCIÃ“N âš ï¸  Dos presuntos delincuentes fuero...</td>\n",
       "      <td>https://twitter.com/noticiasru593/status/14767...</td>\n",
       "      <td>['SamborondÃ³n']</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476682498826452992</td>\n",
       "      <td>2021-12-30 22:31:57+00:00</td>\n",
       "      <td>Y si los hackean @PoliciaEcuador y me terminan...</td>\n",
       "      <td>https://twitter.com/lujacome/status/1476682498...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1476675906903191552</td>\n",
       "      <td>2021-12-30 22:05:46+00:00</td>\n",
       "      <td>@EPattoA @PoliciaEcuador Les encanta quedar as...</td>\n",
       "      <td>https://twitter.com/Rengel_A_G/status/14766759...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476672423076499456</td>\n",
       "      <td>2021-12-30 21:51:55+00:00</td>\n",
       "      <td>ğŸ™ŒğŸ“¢ CONSULTA CONECTADOS | El asalto del aÃ±o fue...</td>\n",
       "      <td>https://twitter.com/conectados_ec/status/14766...</td>\n",
       "      <td>['Santiago']</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476669177104588806</td>\n",
       "      <td>2021-12-30 21:39:01+00:00</td>\n",
       "      <td>@EPattoA @PoliciaEcuador 14 aÃ±os robando Que q...</td>\n",
       "      <td>https://twitter.com/cesar197449/status/1476669...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                       date  \\\n",
       "0  1476701687070998535  2021-12-30 23:48:12+00:00   \n",
       "1  1476682498826452992  2021-12-30 22:31:57+00:00   \n",
       "2  1476675906903191552  2021-12-30 22:05:46+00:00   \n",
       "3  1476672423076499456  2021-12-30 21:51:55+00:00   \n",
       "4  1476669177104588806  2021-12-30 21:39:01+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  #ATENCIÃ“N âš ï¸  Dos presuntos delincuentes fuero...   \n",
       "1  Y si los hackean @PoliciaEcuador y me terminan...   \n",
       "2  @EPattoA @PoliciaEcuador Les encanta quedar as...   \n",
       "3  ğŸ™ŒğŸ“¢ CONSULTA CONECTADOS | El asalto del aÃ±o fue...   \n",
       "4  @EPattoA @PoliciaEcuador 14 aÃ±os robando Que q...   \n",
       "\n",
       "                                                 url         location label  \n",
       "0  https://twitter.com/noticiasru593/status/14767...  ['SamborondÃ³n']  robo  \n",
       "1  https://twitter.com/lujacome/status/1476682498...               []  robo  \n",
       "2  https://twitter.com/Rengel_A_G/status/14766759...               []  robo  \n",
       "3  https://twitter.com/conectados_ec/status/14766...     ['Santiago']  robo  \n",
       "4  https://twitter.com/cesar197449/status/1476669...               []  robo  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 598 entries, 0 to 597\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        598 non-null    int64 \n",
      " 1   date      598 non-null    object\n",
      " 2   text      598 non-null    object\n",
      " 3   url       598 non-null    object\n",
      " 4   location  598 non-null    object\n",
      " 5   label     598 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 28.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"./../datasets/dataset_labeled_v2.csv\")\n",
    "# dataset = dataset[293:298].copy()\n",
    "# Load stop words\n",
    "with open(\"stop_words.txt\", \"r\") as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "print(stop_words)\n",
    "display(dataset.head())\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaner quevedo  hace pocos minutos se registrÃ³ un caso de raptÃ³ varios sujetos secuestraron al dueÃ±o de un local comercial  litardo  ubicado en la parroquia  san_camilo     y el  plan fÃ©nix\n",
      "StopWordsRemover quevedo minutos registrÃ³ caso raptÃ³ sujetos secuestraron dueÃ±o local comercial litardo ubicado parroquia san_camilo plan fÃ©nix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['robaron', 'motorizados', 'parque', 'inglÃ©s', 'a', 'ev', 'calle', 'simÃ³n', 'cÃ¡rdenas', 'joaquÃ­n', 'soto', 'diciembre', 'dÃ³nde', 'denuncia', 'parque', 'inglÃ©s']\n",
      "Entities: [('av calle simÃ³n', 'LOC', 34, 48)]\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['ojos', 'Ã¡guila', 'capta', 'actividad', 'denuncia', 'robo', 'colinas', 'alborada', 'a', 'ev', 'csn', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['upc', 'robo', 'jtm', 'a', 'eh', 'sector', 'corresponde', 'quÃ©', 'marca', 'cel', 'iphone', 'sÃ­', 'rastrear', 'samsung', 'delo', 'perdido', 'aplausos']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['ojos', 'Ã¡guila', 'capta', 'actividad', 'denuncia', 'robo', 'domicilio', 'a', 'ev', 'octubre', 'boyaca', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sistema', 'ojos', 'Ã¡guila', 'capta', 'robo', 'luminaria', 'a', 'ev', 'car', 'los', 'julio', 'arosemena', 'monjas', 'coordinÃ³', 'miembros', 'tomaron', 'procedimiento', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sirve', 'cÃ¡mara', 'colegio', 'francisco', 'orellana', 'estÃ¡n', 'robando', 'llevan', 'a', 'em', 'celular', 'mochila', 'laptop', 'billetera', 'maldita', 'delincuencia', 'pudieron', 'vivo', 'enviar', 'motorizado', 'irassss']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['esmeraldas', 'momentos', 'zona', 'guerra', 'gdo', 'estÃ¡n', 'alborotados', 'robando', 'quemando', 'carros', 'dando', 'se', 'bala', 'quÃ©', 'pasa', 'gente', 'asustada', 'informaciÃ³n', 'oficial', 'deciden', 'encerrar', 'se', 'domicilios']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cÃ¡rcel', 'latacunga', 'crear', 'cÃ¡rcel', 'bandas', 'narco', 'joven', 'delincuente', 'rehabilitar', 'se', 'reintegrar', 'sociedad', 'repetir', 'recuerda', 'robando', 'haciendo', 'se', 'jefe', 'barrio', 'falta', 'oportunidades', 'vemos', 'terminar', 'oportunidad', 'jÃ³venes']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['imaganen', 'se', 'hackeado', 'hackean', 'favorecer', 'principal', 'sospechoso', 'femicidio', 'naomiarcentales', 'surrealista']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['indÃ­gnate', 'llame', 'llegaron', 'domicilio', 'comentar', 'les', 'secuestraron', 'robaron', 'chicas', 'dejando', 'las', 'tiradas', 'callejÃ³n', 'brisasdelrÃ­o', 'indignante', 'peligro', 'hagan']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['indÃ­gnate', 'llame', 'llegaron', 'domicilio', 'comentar', 'les', 'secuestraron', 'robaron', 'chicas', 'dejando', 'las', 'tiradas', 'callejÃ³n', 'brisas', 'rÃ­o', 'indignante', 'peligro', 'hagan']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saquen', 'mierda', 'terroristas', 'mitines', 'llanen', 'lo', 'gas', 'desarmen', 'comprar', 'lo', 'pena', 'muerte', 'permiten', 'presos', 'armas', 'muerte', 'terrorismo']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['autoridades', 'cambiar', 'peste', 'extorsion', 'seguirÃ¡', 'expandiendo', 'se', 'visto', 'ignorancia', 'gente', 'barrios', 'alegra', 'regalan', 'plato', 'lentejas', 'vandÃ¡lico']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sr', 'novita', 'gracias', 'sra', 'palencia', 'gracias', 'sr', 'lofredo', 'tambiÃ©n', 'comiendo', 'pavo', 'pueblo', 'comiendo', 'se', 'trÃ­o', 'incapaces', 'detener']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer quevedo minuto registrar caso raptar sujeto secuestrar dueÃ±o local comercial litardo ubicado parroquia san_camilo plan fÃ©nix\n"
     ]
    }
   ],
   "source": [
    "# pipeline = Pipeline(steps=[\n",
    "#     (\"cleaner\", Cleaner()),\n",
    "#     (\"stop_words_remover\", StopWordsRemover(stop_words=stop_words)),\n",
    "#     (\"lemmatizer\", Lemmatizer()),\n",
    "#     (\"tfidf\", TfidfVectorizer()),\n",
    "# ])\n",
    "\n",
    "df_clean = Cleaner().transform(dataset['text'])\n",
    "print('Cleaner', df_clean.iloc[293])\n",
    "df_sw = StopWordsRemover(stop_words=stop_words).transform(df_clean)\n",
    "print('StopWordsRemover', df_sw.iloc[293])\n",
    "df_lem = Lemmatizer().transform(df_sw)\n",
    "print('Lemmatizer', df_lem.iloc[293])\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "df_tfidf = tfidf_vectorizer.fit_transform(df_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quevedo</td>\n",
       "      <td>0.234918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minuto</td>\n",
       "      <td>0.261482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>registrar</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>caso</td>\n",
       "      <td>0.194523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raptar</td>\n",
       "      <td>0.302892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sujeto</td>\n",
       "      <td>0.221916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>secuestrar</td>\n",
       "      <td>0.183624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dueÃ±o</td>\n",
       "      <td>0.261482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>local</td>\n",
       "      <td>0.234918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>comercial</td>\n",
       "      <td>0.240241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>litardo</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ubicado</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>parroquia</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>san</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>camilo</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plan</td>\n",
       "      <td>0.211832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fÃ©nix</td>\n",
       "      <td>0.225849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variable     value\n",
       "0      quevedo  0.234918\n",
       "1       minuto  0.261482\n",
       "2    registrar  0.271567\n",
       "3         caso  0.194523\n",
       "4       raptar  0.302892\n",
       "5       sujeto  0.221916\n",
       "6   secuestrar  0.183624\n",
       "7        dueÃ±o  0.261482\n",
       "8        local  0.234918\n",
       "9    comercial  0.240241\n",
       "10     litardo  0.271567\n",
       "11     ubicado  0.271567\n",
       "12   parroquia  0.271567\n",
       "13         san  0.000000\n",
       "14      camilo  0.000000\n",
       "15        plan  0.211832\n",
       "16       fÃ©nix  0.225849"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(tfidf_vectorizer.vocabulary_)\n",
    "# print(tfidf_vectorizer.get_feature_names_out())\n",
    "# print(df_tfidf.toarray())\n",
    "\n",
    "df_results = pd.DataFrame(df_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df_results[293:294][['quevedo', 'minuto', 'registrar', 'caso', 'raptar', 'sujeto', 'secuestrar', 'dueÃ±o', 'local', 'comercial', 'litardo', 'ubicado', 'parroquia', 'san', 'camilo', 'plan', 'fÃ©nix']].melt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quevedo => 0.234918, minuto => 0.261482, registrar => 0.271567, caso => 0.194523, raptar => 0.302892, sujeto => 0.221916, secuestrar => 0.183624, dueÃ±o => 0.261482, local => 0.234918, comercial => 0.240241, litardo => 0.271567, ubicado => 0.271567, parroquia => 0.271567, san => 0.000000, camilo => 0.000000, plan => 0.211832, fÃ©nix => 0.225849"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
