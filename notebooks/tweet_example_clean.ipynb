{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "# Custom transformers\n",
    "from custom_transformers import Cleaner, StopWordsRemover, Lemmatizer\n",
    "# TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download the Spanish model\n",
    "# import stanza\n",
    "# stanza.download(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'actualmente', 'acuerdo', 'adelante', 'ademas', 'ademÃ¡s', 'adrede', 'afirmÃ³', 'agregÃ³', 'ahi', 'ahora', 'ahÃ\\xad', 'al', 'algo', 'alguna', 'algunas', 'alguno', 'algunos', 'algÃºn', 'alli', 'allÃ\\xad', 'alrededor', 'ambos', 'ampleamos', 'antano', 'antaÃ±o', 'ante', 'anterior', 'antes', 'apenas', 'aproximadamente', 'aquel', 'aquella', 'aquellas', 'aquello', 'aquellos', 'aqui', 'aquÃ©l', 'aquÃ©lla', 'aquÃ©llas', 'aquÃ©llos', 'aquÃ\\xad', 'arriba', 'arribaabajo', 'asegurÃ³', 'asi', 'asÃ\\xad', 'atras', 'aun', 'aunque', 'ayer', 'aÃ±adiÃ³', 'aÃºn', 'b', 'bajo', 'bastante', 'bien', 'breve', 'buen', 'buena', 'buenas', 'bueno', 'buenos', 'c', 'cada', 'casi', 'cerca', 'cierta', 'ciertas', 'cierto', 'ciertos', 'cinco', 'claro', 'comentÃ³', 'como', 'con', 'conmigo', 'conocer', 'conseguimos', 'conseguir', 'considera', 'considerÃ³', 'consigo', 'consigue', 'consiguen', 'consigues', 'contigo', 'contra', 'cosas', 'creo', 'cual', 'cuales', 'cualquier', 'cuando', 'cuanta', 'cuantas', 'cuanto', 'cuantos', 'cuatro', 'cuenta', 'cuÃ¡l', 'cuÃ¡les', 'cuÃ¡ndo', 'cuÃ¡nta', 'cuÃ¡ntas', 'cuÃ¡nto', 'cuÃ¡ntos', 'cÃ³mo', 'd', 'da', 'dado', 'dan', 'dar', 'de', 'debajo', 'debe', 'deben', 'debido', 'decir', 'dejÃ³', 'del', 'delante', 'demasiado', 'demÃ¡s', 'dentro', 'deprisa', 'desde', 'despacio', 'despues', 'despuÃ©s', 'detras', 'detrÃ¡s', 'dia', 'dias', 'dice', 'dicen', 'dicho', 'dieron', 'diferente', 'diferentes', 'dijeron', 'dijo', 'dio', 'donde', 'dos', 'durante', 'dÃ\\xada', 'dÃ\\xadas', 'dÃ³nde', 'e', 'ejemplo', 'el', 'ella', 'ellas', 'ello', 'ellos', 'embargo', 'empleais', 'emplean', 'emplear', 'empleas', 'empleo', 'en', 'encima', 'encuentra', 'enfrente', 'enseguida', 'entonces', 'entre', 'era', 'erais', 'eramos', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estais', 'estamos', 'estan', 'estando', 'estar', 'estaremos', 'estarÃ¡', 'estarÃ¡n', 'estarÃ¡s', 'estarÃ©', 'estarÃ©is', 'estarÃ\\xada', 'estarÃ\\xadais', 'estarÃ\\xadamos', 'estarÃ\\xadan', 'estarÃ\\xadas', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviÃ©ramos', 'estuviÃ©semos', 'estuvo', 'estÃ¡', 'estÃ¡bamos', 'estÃ¡is', 'estÃ¡n', 'estÃ¡s', 'estÃ©', 'estÃ©is', 'estÃ©n', 'estÃ©s', 'ex', 'excepto', 'existe', 'existen', 'explicÃ³', 'expresÃ³', 'f', 'fin', 'final', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuÃ©ramos', 'fuÃ©semos', 'g', 'general', 'gran', 'grandes', 'gueno', 'h', 'ha', 'haber', 'habia', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habla', 'hablan', 'habremos', 'habrÃ¡', 'habrÃ¡n', 'habrÃ¡s', 'habrÃ©', 'habrÃ©is', 'habrÃ\\xada', 'habrÃ\\xadais', 'habrÃ\\xadamos', 'habrÃ\\xadan', 'habrÃ\\xadas', 'habÃ©is', 'habÃ\\xada', 'habÃ\\xadais', 'habÃ\\xadamos', 'habÃ\\xadan', 'habÃ\\xadas', 'hace', 'haceis', 'hacemos', 'hacen', 'hacer', 'hacerlo', 'haces', 'hacia', 'haciendo', 'hago', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayÃ¡is', 'he', 'hecho', 'hemos', 'hicieron', 'hizo', 'horas', 'hoy', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiÃ©ramos', 'hubiÃ©semos', 'hubo', 'i', 'igual', 'incluso', 'indicÃ³', 'informo', 'informÃ³', 'intenta', 'intentais', 'intentamos', 'intentan', 'intentar', 'intentas', 'intento', 'ir', 'j', 'junto', 'k', 'l', 'la', 'lado', 'largo', 'las', 'le', 'lejos', 'les', 'llegÃ³', 'lleva', 'llevar', 'lo', 'los', 'luego', 'lugar', 'm', 'mal', 'manera', 'manifestÃ³', 'mas', 'mayor', 'me', 'mediante', 'medio', 'mejor', 'mencionÃ³', 'menos', 'menudo', 'mi', 'mia', 'mias', 'mientras', 'mio', 'mios', 'mis', 'misma', 'mismas', 'mismo', 'mismos', 'modo', 'momento', 'mucha', 'muchas', 'mucho', 'muchos', 'muy', 'mÃ¡s', 'mÃ\\xad', 'mÃ\\xada', 'mÃ\\xadas', 'mÃ\\xado', 'mÃ\\xados', 'n', 'nada', 'nadie', 'ni', 'ninguna', 'ningunas', 'ninguno', 'ningunos', 'ningÃºn', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'nueva', 'nuevas', 'nuevo', 'nuevos', 'nunca', 'o', 'ocho', 'os', 'otra', 'otras', 'otro', 'otros', 'p', 'pais', 'para', 'parece', 'parte', 'partir', 'pasada', 'pasado', 'paÃ¬s', 'peor', 'pero', 'pesar', 'poca', 'pocas', 'poco', 'pocos', 'podeis', 'podemos', 'poder', 'podria', 'podriais', 'podriamos', 'podrian', 'podrias', 'podrÃ¡', 'podrÃ¡n', 'podrÃ\\xada', 'podrÃ\\xadan', 'poner', 'por', 'por quÃ©', 'porque', 'posible', 'primer', 'primera', 'primero', 'primeros', 'principalmente', 'pronto', 'propia', 'propias', 'propio', 'propios', 'proximo', 'prÃ³ximo', 'prÃ³ximos', 'pudo', 'pueda', 'puede', 'pueden', 'puedo', 'pues', 'q', 'qeu', 'que', 'quedÃ³', 'queremos', 'quien', 'quienes', 'quiere', 'quiza', 'quizas', 'quizÃ¡', 'quizÃ¡s', 'quiÃ©n', 'quiÃ©nes', 'quÃ©', 'r', 'raras', 'realizado', 'realizar', 'realizÃ³', 'repente', 'respecto', 's', 'sabe', 'sabeis', 'sabemos', 'saben', 'saber', 'sabes', 'sal', 'salvo', 'se', 'sea', 'seamos', 'sean', 'seas', 'segun', 'segunda', 'segundo', 'segÃºn', 'seis', 'ser', 'sera', 'seremos', 'serÃ¡', 'serÃ¡n', 'serÃ¡s', 'serÃ©', 'serÃ©is', 'serÃ\\xada', 'serÃ\\xadais', 'serÃ\\xadamos', 'serÃ\\xadan', 'serÃ\\xadas', 'seÃ¡is', 'seÃ±alÃ³', 'si', 'sido', 'siempre', 'siendo', 'siete', 'sigue', 'siguiente', 'sin', 'sino', 'sobre', 'sois', 'sola', 'solamente', 'solas', 'solo', 'solos', 'somos', 'son', 'soy', 'soyos', 'su', 'supuesto', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sÃ©', 'sÃ\\xad', 'sÃ³lo', 't', 'tal', 'tambien', 'tambiÃ©n', 'tampoco', 'tan', 'tanto', 'tarde', 'te', 'temprano', 'tendremos', 'tendrÃ¡', 'tendrÃ¡n', 'tendrÃ¡s', 'tendrÃ©', 'tendrÃ©is', 'tendrÃ\\xada', 'tendrÃ\\xadais', 'tendrÃ\\xadamos', 'tendrÃ\\xadan', 'tendrÃ\\xadas', 'tened', 'teneis', 'tenemos', 'tener', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengÃ¡is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenÃ©is', 'tenÃ\\xada', 'tenÃ\\xadais', 'tenÃ\\xadamos', 'tenÃ\\xadan', 'tenÃ\\xadas', 'tercera', 'ti', 'tiempo', 'tiene', 'tienen', 'tienes', 'toda', 'todas', 'todavia', 'todavÃ\\xada', 'todo', 'todos', 'total', 'trabaja', 'trabajais', 'trabajamos', 'trabajan', 'trabajar', 'trabajas', 'trabajo', 'tras', 'trata', 'travÃ©s', 'tres', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviÃ©ramos', 'tuviÃ©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tÃº', 'u', 'ultimo', 'un', 'una', 'unas', 'uno', 'unos', 'usa', 'usais', 'usamos', 'usan', 'usar', 'usas', 'uso', 'usted', 'ustedes', 'v', 'va', 'vais', 'valor', 'vamos', 'van', 'varias', 'varios', 'vaya', 'veces', 'ver', 'verdad', 'verdadera', 'verdadero', 'vez', 'vosotras', 'vosotros', 'voy', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'w', 'x', 'y', 'ya', 'yo', 'z', 'Ã©l', 'Ã©ramos', 'Ã©sa', 'Ã©sas', 'Ã©se', 'Ã©sos', 'Ã©sta', 'Ã©stas', 'Ã©ste', 'Ã©stos', 'Ãºltima', 'Ãºltimas', 'Ãºltimo', 'Ãºltimos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476701687070998535</td>\n",
       "      <td>2021-12-30 23:48:12+00:00</td>\n",
       "      <td>#ATENCIÓN ⚠️  Dos presuntos delincuentes fuero...</td>\n",
       "      <td>https://twitter.com/noticiasru593/status/14767...</td>\n",
       "      <td>['Samborondón']</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476682498826452992</td>\n",
       "      <td>2021-12-30 22:31:57+00:00</td>\n",
       "      <td>Y si los hackean @PoliciaEcuador y me terminan...</td>\n",
       "      <td>https://twitter.com/lujacome/status/1476682498...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1476675906903191552</td>\n",
       "      <td>2021-12-30 22:05:46+00:00</td>\n",
       "      <td>@EPattoA @PoliciaEcuador Les encanta quedar as...</td>\n",
       "      <td>https://twitter.com/Rengel_A_G/status/14766759...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476672423076499456</td>\n",
       "      <td>2021-12-30 21:51:55+00:00</td>\n",
       "      <td>🙌📢 CONSULTA CONECTADOS | El asalto del año fue...</td>\n",
       "      <td>https://twitter.com/conectados_ec/status/14766...</td>\n",
       "      <td>['Santiago']</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476669177104588806</td>\n",
       "      <td>2021-12-30 21:39:01+00:00</td>\n",
       "      <td>@EPattoA @PoliciaEcuador 14 años robando Que q...</td>\n",
       "      <td>https://twitter.com/cesar197449/status/1476669...</td>\n",
       "      <td>[]</td>\n",
       "      <td>robo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                       date  \\\n",
       "0  1476701687070998535  2021-12-30 23:48:12+00:00   \n",
       "1  1476682498826452992  2021-12-30 22:31:57+00:00   \n",
       "2  1476675906903191552  2021-12-30 22:05:46+00:00   \n",
       "3  1476672423076499456  2021-12-30 21:51:55+00:00   \n",
       "4  1476669177104588806  2021-12-30 21:39:01+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  #ATENCIÓN ⚠️  Dos presuntos delincuentes fuero...   \n",
       "1  Y si los hackean @PoliciaEcuador y me terminan...   \n",
       "2  @EPattoA @PoliciaEcuador Les encanta quedar as...   \n",
       "3  🙌📢 CONSULTA CONECTADOS | El asalto del año fue...   \n",
       "4  @EPattoA @PoliciaEcuador 14 años robando Que q...   \n",
       "\n",
       "                                                 url         location label  \n",
       "0  https://twitter.com/noticiasru593/status/14767...  ['Samborondón']  robo  \n",
       "1  https://twitter.com/lujacome/status/1476682498...               []  robo  \n",
       "2  https://twitter.com/Rengel_A_G/status/14766759...               []  robo  \n",
       "3  https://twitter.com/conectados_ec/status/14766...     ['Santiago']  robo  \n",
       "4  https://twitter.com/cesar197449/status/1476669...               []  robo  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 598 entries, 0 to 597\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        598 non-null    int64 \n",
      " 1   date      598 non-null    object\n",
      " 2   text      598 non-null    object\n",
      " 3   url       598 non-null    object\n",
      " 4   location  598 non-null    object\n",
      " 5   label     598 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 28.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"./../datasets/dataset_labeled_v2.csv\")\n",
    "# dataset = dataset[293:298].copy()\n",
    "# Load stop words\n",
    "with open(\"stop_words.txt\", \"r\") as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "print(stop_words)\n",
    "display(dataset.head())\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaner quevedo  hace pocos minutos se registró un caso de raptó varios sujetos secuestraron al dueño de un local comercial  litardo  ubicado en la parroquia  san_camilo     y el  plan fénix\n",
      "StopWordsRemover quevedo minutos registró caso raptó sujetos secuestraron dueño local comercial litardo ubicado parroquia san_camilo plan fénix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['robaron', 'motorizados', 'parque', 'inglés', 'a', 'ev', 'calle', 'simón', 'cárdenas', 'joaquín', 'soto', 'diciembre', 'dónde', 'denuncia', 'parque', 'inglés']\n",
      "Entities: [('av calle simón', 'LOC', 34, 48)]\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['ojos', 'águila', 'capta', 'actividad', 'denuncia', 'robo', 'colinas', 'alborada', 'a', 'ev', 'csn', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['upc', 'robo', 'jtm', 'a', 'eh', 'sector', 'corresponde', 'qué', 'marca', 'cel', 'iphone', 'sí', 'rastrear', 'samsung', 'delo', 'perdido', 'aplausos']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['ojos', 'águila', 'capta', 'actividad', 'denuncia', 'robo', 'domicilio', 'a', 'ev', 'octubre', 'boyaca', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sistema', 'ojos', 'águila', 'capta', 'robo', 'luminaria', 'a', 'ev', 'car', 'los', 'julio', 'arosemena', 'monjas', 'coordinó', 'miembros', 'tomaron', 'procedimiento', 'portuseguridad']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sirve', 'cámara', 'colegio', 'francisco', 'orellana', 'están', 'robando', 'llevan', 'a', 'em', 'celular', 'mochila', 'laptop', 'billetera', 'maldita', 'delincuencia', 'pudieron', 'vivo', 'enviar', 'motorizado', 'irassss']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['esmeraldas', 'momentos', 'zona', 'guerra', 'gdo', 'están', 'alborotados', 'robando', 'quemando', 'carros', 'dando', 'se', 'bala', 'qué', 'pasa', 'gente', 'asustada', 'información', 'oficial', 'deciden', 'encerrar', 'se', 'domicilios']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cárcel', 'latacunga', 'crear', 'cárcel', 'bandas', 'narco', 'joven', 'delincuente', 'rehabilitar', 'se', 'reintegrar', 'sociedad', 'repetir', 'recuerda', 'robando', 'haciendo', 'se', 'jefe', 'barrio', 'falta', 'oportunidades', 'vemos', 'terminar', 'oportunidad', 'jóvenes']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['imaganen', 'se', 'hackeado', 'hackean', 'favorecer', 'principal', 'sospechoso', 'femicidio', 'naomiarcentales', 'surrealista']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['indígnate', 'llame', 'llegaron', 'domicilio', 'comentar', 'les', 'secuestraron', 'robaron', 'chicas', 'dejando', 'las', 'tiradas', 'callejón', 'brisasdelrío', 'indignante', 'peligro', 'hagan']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['indígnate', 'llame', 'llegaron', 'domicilio', 'comentar', 'les', 'secuestraron', 'robaron', 'chicas', 'dejando', 'las', 'tiradas', 'callejón', 'brisas', 'río', 'indignante', 'peligro', 'hagan']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['saquen', 'mierda', 'terroristas', 'mitines', 'llanen', 'lo', 'gas', 'desarmen', 'comprar', 'lo', 'pena', 'muerte', 'permiten', 'presos', 'armas', 'muerte', 'terrorismo']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['autoridades', 'cambiar', 'peste', 'extorsion', 'seguirá', 'expandiendo', 'se', 'visto', 'ignorancia', 'gente', 'barrios', 'alegra', 'regalan', 'plato', 'lentejas', 'vandálico']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "d:\\GitHub\\tfm\\.venv\\lib\\site-packages\\spacy\\language.py:1037: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['sr', 'novita', 'gracias', 'sra', 'palencia', 'gracias', 'sr', 'lofredo', 'también', 'comiendo', 'pavo', 'pueblo', 'comiendo', 'se', 'trío', 'incapaces', 'detener']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer quevedo minuto registrar caso raptar sujeto secuestrar dueño local comercial litardo ubicado parroquia san_camilo plan fénix\n"
     ]
    }
   ],
   "source": [
    "# pipeline = Pipeline(steps=[\n",
    "#     (\"cleaner\", Cleaner()),\n",
    "#     (\"stop_words_remover\", StopWordsRemover(stop_words=stop_words)),\n",
    "#     (\"lemmatizer\", Lemmatizer()),\n",
    "#     (\"tfidf\", TfidfVectorizer()),\n",
    "# ])\n",
    "\n",
    "df_clean = Cleaner().transform(dataset['text'])\n",
    "print('Cleaner', df_clean.iloc[293])\n",
    "df_sw = StopWordsRemover(stop_words=stop_words).transform(df_clean)\n",
    "print('StopWordsRemover', df_sw.iloc[293])\n",
    "df_lem = Lemmatizer().transform(df_sw)\n",
    "print('Lemmatizer', df_lem.iloc[293])\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "df_tfidf = tfidf_vectorizer.fit_transform(df_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quevedo</td>\n",
       "      <td>0.234918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minuto</td>\n",
       "      <td>0.261482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>registrar</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>caso</td>\n",
       "      <td>0.194523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raptar</td>\n",
       "      <td>0.302892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sujeto</td>\n",
       "      <td>0.221916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>secuestrar</td>\n",
       "      <td>0.183624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dueño</td>\n",
       "      <td>0.261482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>local</td>\n",
       "      <td>0.234918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>comercial</td>\n",
       "      <td>0.240241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>litardo</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ubicado</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>parroquia</td>\n",
       "      <td>0.271567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>san</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>camilo</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plan</td>\n",
       "      <td>0.211832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fénix</td>\n",
       "      <td>0.225849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variable     value\n",
       "0      quevedo  0.234918\n",
       "1       minuto  0.261482\n",
       "2    registrar  0.271567\n",
       "3         caso  0.194523\n",
       "4       raptar  0.302892\n",
       "5       sujeto  0.221916\n",
       "6   secuestrar  0.183624\n",
       "7        dueño  0.261482\n",
       "8        local  0.234918\n",
       "9    comercial  0.240241\n",
       "10     litardo  0.271567\n",
       "11     ubicado  0.271567\n",
       "12   parroquia  0.271567\n",
       "13         san  0.000000\n",
       "14      camilo  0.000000\n",
       "15        plan  0.211832\n",
       "16       fénix  0.225849"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(tfidf_vectorizer.vocabulary_)\n",
    "# print(tfidf_vectorizer.get_feature_names_out())\n",
    "# print(df_tfidf.toarray())\n",
    "\n",
    "df_results = pd.DataFrame(df_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df_results[293:294][['quevedo', 'minuto', 'registrar', 'caso', 'raptar', 'sujeto', 'secuestrar', 'dueño', 'local', 'comercial', 'litardo', 'ubicado', 'parroquia', 'san', 'camilo', 'plan', 'fénix']].melt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quevedo => 0.234918, minuto => 0.261482, registrar => 0.271567, caso => 0.194523, raptar => 0.302892, sujeto => 0.221916, secuestrar => 0.183624, dueño => 0.261482, local => 0.234918, comercial => 0.240241, litardo => 0.271567, ubicado => 0.271567, parroquia => 0.271567, san => 0.000000, camilo => 0.000000, plan => 0.211832, fénix => 0.225849"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
